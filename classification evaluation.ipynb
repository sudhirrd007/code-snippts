{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='another_cell'> CLICK </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If test data is imbalanced than don't use accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can use inbilit \"score\" method of classifier\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# below shows total unmatched points\n",
    "accuracy_score(y_test, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use probability prediction values to compare two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "# dataframe representation\n",
    "round(pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TPR = TP / P <-- High\n",
    "TNR = TN / N <-- High\n",
    "FPR = FP / N <-- Low\n",
    "FNR = FN / P <-- Low\n",
    "\n",
    "Any dumb model may also be a good model and we can check it through upper 4 equations\n",
    "\n",
    "Upper equation's importance mainly depends on domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "confusion_matrix(y_true, y_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "don't forgot using skleran, true values --> vertical side and predicted --> horizontal side\n",
    " --- pred ---\n",
    "     0 1 2 3\n",
    " t 0\n",
    " r 1\n",
    " u 2\n",
    " e 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confusion_heatmap(y_test, y_pred, label_mapping=None, normalize=None):\n",
    "    labels = np.unique(np.concatenate((np.unique(y_test), np.unique(y_pred)), axis=0))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels, normalize=normalize)\n",
    "    \n",
    "    mapping = labels\n",
    "    if(label_mapping):\n",
    "        mapping = [name_mapping[l] for l in labels]\n",
    "\n",
    "    d = pd.DataFrame(cm)\n",
    "    d.columns = mapping\n",
    "    d.index = mapping\n",
    "\n",
    "    sns.heatmap(d, annot=True, fmt=\".4g\", cmap=\"Blues\", )\n",
    "    plt.ylabel('True label',fontsize=12)\n",
    "    plt.xlabel('Predicted label',fontsize=12)\n",
    "    plt.show();\n",
    "# label_mapping = {0:\"No\", 1:\"Yes\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_dataframe(y_test, y_pred, y_prob):\n",
    "    y_pred = pd.Series(y_pred, index = y_test.index)\n",
    "    y_prob = pd.DataFrame(y_prob, index = y_test.index)\n",
    "    \n",
    "    df = pd.concat([y_test, y_pred, y_prob], axis=1)\n",
    "    df.columns = [\"actual\", \"prediction\", \"0\", \"1\"]\n",
    "    return df\n",
    "# y_prob = gnb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_dataframe(y_test, y_pred, y_prob):\n",
    "    y_pred = pd.Series(y_pred, index = y_test.index)\n",
    "    y_prob = pd.DataFrame(y_prob, index = y_test.index)\n",
    "    \n",
    "    df = pd.concat([y_test, y_pred, y_prob], axis=1)\n",
    "    df.columns = [\"actual\", \"prediction\", \"0\", \"1\"]\n",
    "    df['new_prediction'] = df['0'].map(lambda x: 0 if x >= 0.6 else 1)\n",
    "    return df\n",
    "# y_prob = gnb.predict_proba(X_test)\n",
    "#>>> dataframe[(dataframe[\"Actual\"] == 1) & (dataframe[\"new_prediction\"] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def brute_force_clf(X, y):\n",
    "    scores = []\n",
    "    randoms = [s for s in range(100)]\n",
    "    for r in tqdm(randoms):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=r);\n",
    "        \n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train, y_train)\n",
    "        scores.append(gnb.score(X_test, y_test))\n",
    "        \n",
    "    df = {\"random_state\": randoms, \"score\": scores}\n",
    "    df = pd.DataFrame(df).sort_values(by=\"score\", ascending=False)\n",
    "    return df\n",
    "\n",
    "brute_force_clf(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another Cell](#another_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
