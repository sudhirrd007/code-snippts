{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If test data is imbalanced than don't use accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use inbilit \"score\" method of classifier\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# below shows total unmatched points\n",
    "accuracy_score(y_test, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use probability prediction values to compare two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "# dataframe representation\n",
    "round(pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TPR = TP / P <-- High\n",
    "TNR = TN / N <-- High\n",
    "FPR = FP / N <-- Low\n",
    "FNR = FN / P <-- Low\n",
    "\n",
    "Any dumb model may also be a good model and we can check it through upper 4 equations\n",
    "\n",
    "Upper equation's importance mainly depends on domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "confusion_matrix(y_true, y_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "don't forgot using skleran, true values --> vertical side and predicted --> horizontal side\n",
    " --- pred ---\n",
    "     0 1 2 3\n",
    " t 0\n",
    " r 1\n",
    " u 2\n",
    " e 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = np.unique(np.concatenate((np.unique(y_test), np.unique(y_pred)), axis=0))\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "d = pd.DataFrame(cm)\n",
    "d.columns = labels\n",
    "d.index = labels\n",
    "\n",
    "sns.heatmap(d, annot=True, fmt=\".4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
